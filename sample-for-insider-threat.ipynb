{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":454382,"sourceType":"datasetVersion","datasetId":207596},{"sourceId":11200903,"sourceType":"datasetVersion","datasetId":6983068}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install psutil pyyaml tensorflow imbalanced-learn==0.9.1 xgboost==1.6.1\n!pip install flask-cors | cat\nimport os\nimport psutil\nimport sys\nimport time\nimport logging\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO, \n                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\nlogger = logging.getLogger('kaggle-notebook')\n\ndef print_memory_usage():\n    memory_info = psutil.virtual_memory()\n    print(f\"Memory usage: {memory_info.percent}%\")\n    print(f\"Available: {memory_info.available / (1024 ** 2):.1f} MB / {memory_info.total / (1024 ** 2):.1f} MB\")\n\nprint(\"\\n=== Initial System Information ===\")\nprint_memory_usage()\n\nprint(\"\\n=== Setting up directories ===\")\nfor dir_path in [\"data/raw\", \"data/interim\", \"data/processed\", \n                 \"models/trained_models\", \"reports\", \"predictions\", \n                 \"deployment\", \"visualizations\"]:\n    os.makedirs(dir_path, exist_ok=True)\n    print(f\"Directory {dir_path} ready\")\n\nsys.path.append('/kaggle/input/insider-threat-detection/Insider-threat-detection')\n\nprint(\"\\n=== Checking for example data ===\")\nexample_data_dir = \"/kaggle/input/insider-threat-detection/Insider-threat-detection/example_data\"\nif os.path.exists(example_data_dir):\n    import shutil\n    print(f\"Copying example data from {example_data_dir}\")\n    for filename in os.listdir(example_data_dir):\n        src_path = os.path.join(example_data_dir, filename)\n        dest_path = os.path.join(\"data/raw\", filename)\n        if os.path.isfile(src_path) and not os.path.exists(dest_path):\n            shutil.copy(src_path, dest_path)\n            print(f\"Copied {filename} to data/raw/\")\nelse:\n    print(\"Example data not found. Using synthetic data.\")\n    \n    print(\"Generating synthetic data...\")\n    try:\n        import numpy as np\n        \n        np.random.seed(42)\n        n_users = 100\n        n_days = 30\n        n_records = n_users * n_days\n        \n        # User IDs\n        user_ids = [f\"U{i:04d}\" for i in range(n_users)]\n        \n        # Generate log data\n        log_data = {\n            \"timestamp\": pd.date_range(start=\"2024-01-01\", periods=n_records),\n            \"user_id\": np.random.choice(user_ids, n_records),\n            \"pc\": np.random.choice([f\"PC{i:03d}\" for i in range(50)], n_records),\n            \"activity\": np.random.choice([\"logon\", \"logoff\", \"connect\", \"disconnect\"], n_records),\n            \"resource\": np.random.choice([\"file_server\", \"email\", \"web\", \"database\"], n_records)\n        }\n        log_df = pd.DataFrame(log_data)\n        os.makedirs(\"data/raw\", exist_ok=True)\n        log_df.to_csv(\"data/raw/synthetic_logs.csv\", index=False)\n        print(f\"Created synthetic log data: {len(log_df)} records\")\n        \n        email_data = {\n            \"timestamp\": pd.date_range(start=\"2024-01-01\", periods=n_records//2),\n            \"user_id\": np.random.choice(user_ids, n_records//2),\n            \"recipient\": np.random.choice(user_ids, n_records//2),\n            \"subject\": np.random.choice([\"Meeting\", \"Report\", \"Update\", \"Question\", \"Important\"], n_records//2),\n            \"content\": np.random.choice([\"Please review\", \"Let's discuss\", \"Confidential information\", \n                                         \"Project update\", \"Need your input\"], n_records//2)\n        }\n        email_df = pd.DataFrame(email_data)\n        email_df.to_csv(\"data/raw/synthetic_emails.csv\", index=False)\n        print(f\"Created synthetic email data: {len(email_df)} records\")\n        \n        # Generate file access data\n        file_data = {\n            \"timestamp\": pd.date_range(start=\"2024-01-01\", periods=n_records//3),\n            \"user_id\": np.random.choice(user_ids, n_records//3),\n            \"filename\": np.random.choice([f\"file_{i}.txt\" for i in range(20)], n_records//3),\n            \"access_type\": np.random.choice([\"read\", \"write\", \"modify\", \"delete\"], n_records//3),\n            \"file_path\": np.random.choice([\"/documents\", \"/reports\", \"/financial\", \"/hr\"], n_records//3)\n        }\n        file_df = pd.DataFrame(file_data)\n        file_df.to_csv(\"data/raw/synthetic_files.csv\", index=False)\n        print(f\"Created synthetic file access data: {len(file_df)} records\")\n    except Exception as e:\n        print(f\"Error generating synthetic data: {str(e)}\")\n\nprint(\"\\n=== Fixing matplotlib style in evaluate.py ===\")\ntry:\n    eval_file = '/kaggle/input/insider-threat-detection/Insider-threat-detection/models/evaluate.py'\n    with open(eval_file, 'r') as f:\n        contents = f.read()\n\n    if 'seaborn-v0_8-darkgrid' in contents and 'try:' not in contents[:500]:\n        fixed_style = \"\"\"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Update the deprecated style\ntry:\n    plt.style.use('seaborn-v0_8-darkgrid')\nexcept:\n    try:\n        plt.style.use('seaborn-darkgrid')\n    except:\n        plt.style.use('default')\n        sns.set_style('darkgrid')  # Fallback to basic seaborn style\n\"\"\"\n        with open('fixed_evaluate.py', 'w') as f:\n            f.write(fixed_style + contents[contents.find('\\n', contents.find('plt.style.use')):])\n        print(\"Created fixed_evaluate.py\")\nexcept Exception as e:\n    print(f\"Error fixing matplotlib style: {str(e)}\")\n\nprint(\"\\n=== Setting up Kaggle scripts ===\")\ntry:\n    import shutil\n    shutil.copy(\"/kaggle/input/insider-threat-detection/Insider-threat-detection/kaggle/kaggle_run.py\", \n                \"kaggle_run.py\")\n    print(\"Copied kaggle_run.py to working directory\")\n    \n    shutil.copy(\"/kaggle/input/insider-threat-detection/Insider-threat-detection/kaggle/kaggle_deploy.py\", \n                \"kaggle_deploy.py\")\n    print(\"Copied kaggle_deploy.py to working directory\")\nexcept Exception as e:\n    print(f\"Error copying Kaggle scripts: {str(e)}\")\n\nprint(\"\\n=== Starting Insider Threat Detection ML Lifecycle ===\")\nprint(\"=== Using optimized hyperparameters with improved error handling ===\")\nprint(\"\")\n\nstart_time = time.time()\n\n!python /kaggle/input/insider-threat-detection/Insider-threat-detection/kaggle/kaggle_run.py --debug --sample 0.1\n\nruntime = time.time() - start_time\nhours, remainder = divmod(runtime, 3600)\nminutes, seconds = divmod(remainder, 60)\nprint(f\"\\n=== Training completed in {int(hours)}h {int(minutes)}m {int(seconds)}s ===\")\n\nprint(\"\\n=== Final Memory Usage ===\")\nprint_memory_usage()\n\nprint(\"\\n=== Running visualization script ===\")\ntry:\n    from visualize_results import main as visualize_main\n    visualize_main()\nexcept Exception as e:\n    print(f\"Error running visualization script: {str(e)}\")\n    \n    metrics_path = \"reports/model_metrics.csv\"\n    if os.path.exists(metrics_path):\n        try:\n            metrics_df = pd.read_csv(metrics_path)\n            print(\"\\nModel Performance Metrics:\")\n            display(metrics_df)\n        except Exception as viz_error:\n            print(f\"Error displaying metrics: {str(viz_error)}\")\n\nprint(\"\\n=== Displaying Results ===\")\ntry:\n    if os.path.exists(\"visualizations\"):\n        viz_files = list(Path(\"visualizations\").glob(\"*.png\"))\n        if viz_files:\n            print(f\"Found {len(viz_files)} visualization files\")\n            \n            # Display combined metrics if available\n            combined_metrics = \"visualizations/combined_metrics.png\"\n            if os.path.exists(combined_metrics):\n                plt.figure(figsize=(10, 6))\n                img = plt.imread(combined_metrics)\n                plt.imshow(img)\n                plt.axis(\"off\")\n                plt.title(\"Model Performance Comparison\")\n                plt.show()\n            \n            conf_matrices = \"visualizations/all_confusion_matrices.png\"\n            if os.path.exists(conf_matrices):\n                plt.figure(figsize=(12, 8))\n                img = plt.imread(conf_matrices)\n                plt.imshow(img)\n                plt.axis(\"off\")\n                plt.title(\"Confusion Matrices\")\n                plt.show()\n            \n            rf_importance = \"visualizations/random_forest_feature_importance.png\"\n            if os.path.exists(rf_importance):\n                plt.figure(figsize=(10, 8))\n                img = plt.imread(rf_importance)\n                plt.imshow(img)\n                plt.axis(\"off\")\n                plt.title(\"Random Forest Feature Importance\")\n                plt.show()\n        else:\n            print(\"No visualization files found\")\nexcept Exception as e:\n    print(f\"Error displaying visualizations: {str(e)}\")\n\nprint(\"\\n=== Training and Evaluation Complete ===\")\nprint(\"Check reports/ directory for detailed evaluation results\")\nprint(\"Check visualizations/ directory for performance visualizations\") ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-28T23:53:02.153932Z","iopub.execute_input":"2025-03-28T23:53:02.154590Z","iopub.status.idle":"2025-03-28T23:53:39.960404Z","shell.execute_reply.started":"2025-03-28T23:53:02.154549Z","shell.execute_reply":"2025-03-28T23:53:39.958761Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.1)\nRequirement already satisfied: imbalanced-learn==0.9.1 in /usr/local/lib/python3.10/dist-packages (0.9.1)\nRequirement already satisfied: xgboost==1.6.1 in /usr/local/lib/python3.10/dist-packages (1.6.1)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.9.1) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.9.1) (1.13.1)\nRequirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.9.1) (1.2.2)\nRequirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.9.1) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn==0.9.1) (3.5.0)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.1)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.2)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (75.1.0)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.68.1)\nRequirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.1)\nRequirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.5.0)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.9.4)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.9.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.9.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.9.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.9.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.9.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17.3->imbalanced-learn==0.9.1) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn==0.9.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17.3->imbalanced-learn==0.9.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->imbalanced-learn==0.9.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17.3->imbalanced-learn==0.9.1) (2024.2.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.19.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17.3->imbalanced-learn==0.9.1) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\nRequirement already satisfied: flask-cors in /usr/local/lib/python3.10/dist-packages (5.0.1)\nRequirement already satisfied: flask>=0.9 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (3.1.0)\nRequirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.10/dist-packages (from flask-cors) (3.1.3)\nRequirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.10/dist-packages (from flask>=0.9->flask-cors) (3.1.4)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.10/dist-packages (from flask>=0.9->flask-cors) (2.2.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from flask>=0.9->flask-cors) (8.1.7)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.10/dist-packages (from flask>=0.9->flask-cors) (1.9.0)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug>=0.7->flask-cors) (3.0.2)\n\n=== Initial System Information ===\nMemory usage: 3.8%\nAvailable: 30872.0 MB / 32102.9 MB\n\n=== Setting up directories ===\nDirectory data/raw ready\nDirectory data/interim ready\nDirectory data/processed ready\nDirectory models/trained_models ready\nDirectory reports ready\nDirectory predictions ready\nDirectory deployment ready\nDirectory visualizations ready\n\n=== Checking for example data ===\nExample data not found. Using synthetic data.\nGenerating synthetic data...\nCreated synthetic log data: 3000 records\nCreated synthetic email data: 1500 records\nCreated synthetic file access data: 1000 records\n\n=== Fixing matplotlib style in evaluate.py ===\nCreated fixed_evaluate.py\n\n=== Setting up Kaggle scripts ===\nCopied kaggle_run.py to working directory\nCopied kaggle_deploy.py to working directory\n\n=== Starting Insider Threat Detection ML Lifecycle ===\n=== Using optimized hyperparameters with improved error handling ===\n\n\n====================================================\nðŸ”„ Starting Insider Threat Detection ML Lifecycle on Kaggle\n====================================================\nWorking directory: /kaggle/working\nSource directory: /kaggle/input/insider-threat-detection/Insider-threat-detection\nModels to train: ['random_forest', 'xgboost', 'logistic_regression']\n====================================================\n\n\nðŸ“Š Step 1: Data Preprocessing\n----------------------------------------------------\n/usr/local/lib/python3.10/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n  warnings.warn(\"The twython library has not been installed. \"\nProcessing email content: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1500/1500 [00:00<00:00, 11673.89it/s]\nData preprocessing completed successfully\n\nðŸ§ª Step 2: Feature Engineering\n----------------------------------------------------\nImporting feature engineering modules...\nUsing data sample ratio: 0.1\nLoading log data: /kaggle/working/data/interim/synthetic_logs_processed.csv\nSampled log data: 300 records\nLoading email data: /kaggle/working/data/interim/synthetic_emails_processed.csv\nSampled email data: 150 records\nLoading file data: /kaggle/working/data/interim/synthetic_files_processed.csv\nSampled file data: 100 records\nExtracting log features...\nProcessing users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 95/95 [00:04<00:00, 19.09it/s]\nExtracted log features: 300 records\nExtracting email features...\nProcessing email senders:   0%|                          | 0/69 [00:00<?, ?it/s]\nExtracting network features...\nExtracted network features: 0 records\nExtracting file features...\nProcessing file access users: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 60/60 [00:00<00:00, 96.67it/s]\nExtracted file features: 100 records\nCreating user profiles...\nCreated user profiles: 400 records\nAdding 'insider_threat' target column...\nMarked 20 records as potential threats\nSaving features...\nSaved user profiles: 400 records\nCreated train/test split: 320 train, 80 test\n\nðŸ§  Step 3: Model Training\n----------------------------------------------------\n2025-03-28 23:53:22.700670: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-28 23:53:22.729616: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-28 23:53:22.738024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSuccessfully imported ModelTrainer from train\nTraining models with optimized hyperparameters: ['random_forest', 'xgboost', 'logistic_regression']\nTraining random_forest...\nFitting 5 folds for each of 1 candidates, totalling 5 fits\nTrained Random Forest with parameters: {'n_estimators': [300], 'max_depth': [15], 'min_samples_split': [5], 'min_samples_leaf': [2], 'max_features': ['sqrt'], 'bootstrap': [True], 'class_weight': ['balanced'], 'random_state': [42]}\nTrained random_forest successfully\nTraining xgboost...\nTraining XGBoost directly with CPU-only parameters\nError getting driver and runtime versions:\n\nstdout:\n\n\n\nstderr:\n\nTraceback (most recent call last):\n  File \"<string>\", line 4, in <module>\n  File \"/usr/local/lib/python3.10/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 314, in __getattr__\n    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\nnumba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n\nCUDA driver library cannot be found.\nIf you are sure that a CUDA driver is installed,\ntry setting environment variable NUMBA_CUDA_DRIVER\nwith the file path of the CUDA driver shared library.\n:\n\n\nNot patching Numba\nFailed to dlopen libcuda.so.1\nFunction \"cuInit\" not found\n[23:53:35] WARNING: ../src/learner.cc:627: \nParameters: { \"device\" } might not be used.\n\n  This could be a false alarm, with some parameters getting used by language bindings but\n  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n  but getting flagged wrongly here. Please open an issue if you find any such cases.\n\n\nTrained XGBoost with CPU-only parameters\nTrained XGBoost successfully\nTrained xgboost successfully\nTraining logistic_regression...\nUsing default parameters for logistic regression\nTrained Logistic Regression with optimized parameters\nTrained logistic_regression successfully\nTraining ensemble model with optimized weights...\nTrained ensemble successfully\nModel training completed with optimized parameters\n\nðŸ“ˆ Step 4: Model Evaluation\n----------------------------------------------------\nEvaluating models using data from: /kaggle/working/data/processed/test_features.csv\nEvaluation report generation failed\n\nðŸš€ Step 5: Model Deployment [SKIPPED]\n\n====================================================\nâœ… Machine Learning Lifecycle Complete!\n====================================================\nResults:\n- Trained models saved to: /kaggle/working/models/trained_models\n- Evaluation reports saved to: /kaggle/working/reports\n====================================================\n\nðŸ“Š Creating basic visualizations...\nSaved feature importance visualization for random_forest\n\n=== Training completed in 0h 0m 28s ===\n\n=== Final Memory Usage ===\nMemory usage: 3.7%\nAvailable: 30913.0 MB / 32102.9 MB\n\n=== Running visualization script ===\nError running visualization script: No module named 'visualize_results'\n\n=== Displaying Results ===\nFound 1 visualization files\n\n=== Training and Evaluation Complete ===\nCheck reports/ directory for detailed evaluation results\nCheck visualizations/ directory for performance visualizations\n","output_type":"stream"}],"execution_count":2}]}